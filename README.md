![HipGPT Logo](https://raw.githubusercontent.com/aarnetalman/HipGPT/main/assets/images/hip-hamster.png)

# ğŸ¹ HipGPT: A GPT-2 Implementation in C++ and HIP

*HipGPT is a lightweight GPT-2 style transformer model implemented from scratch in C++ and accelerated with AMD's [HIP API](https://rocm.docs.amd.com/en/latest/understand/hip_api/hip_api.html) for ROCm-enabled GPUs.*  
It includes all the necessary components of a modern language model: a custom BPE tokenizer, a transformer-based GPT model, and GPU kernels for training and inference.

The project is self-contained and designed to be a **clear, educational guide** to the inner workings of large language models.  

ğŸ“– **Documentation:** [https://hipgpt.github.io](https://hipgpt.github.io)

---

## â“ Why HipGPT?

- ğŸ“ **Educational clarity** â€“ written from scratch in modern C++ to expose all the moving parts of a GPT model  
- âš™ï¸ **HIP-first** â€“ showcases AMDâ€™s HIP API for GPU acceleration on ROCm-enabled hardware  
- ğŸ§© **Minimal yet complete** â€“ small enough to understand in full, but includes tokenizer, model, kernels, training, and inference  
- ğŸ”¬ **Research-friendly** â€“ designed as a foundation for experimenting with language models on AMD GPUs  

---

## âœ¨ Features

- ğŸ”¤ **Custom BPE Tokenizer** â€“ built from scratch, trainable on any raw text file  
- ğŸ§  **Transformer Architecture** â€“ GPT-2 style, decoder-only model  
- âš¡ **GPU Acceleration** â€“ custom HIP kernels for matrix multiplication, attention, layer norm, etc.  
- ğŸ“¦ **End-to-End Workflow** â€“ scripts for data download, training, and text generation  
- ğŸ›  **Self-Contained Build System** â€“ CMake-based, automatically fetches required dependencies  

---

## ğŸ“ Model Size

The number of trainable parameters depends on the vocabulary size (`V`) and transformer hyperparameters:

```
Total =
VÂ·E                            (token embeddings)

* SÂ·E                          (positional embeddings)
* LÂ·(4EÂ² + 2EÂ·F + F + 9E)      (per transformer layer: QKV, O, FF1/FF2, LayerNorms)
* EÂ·V + V                      (final projection + bias)

````

Where:
- `E` = embedding dimension  
- `L` = number of layers  
- `H` = number of attention heads (`E` must be divisible by `H`)  
- `F` = feed-forward hidden dimension  
- `V` = vocabulary size  
- `S` = maximum sequence length  

**Default configuration:**  
`E=128, L=2, H=4, F=256, Vâ‰ˆ5000, S=32`  
â¡ï¸ **~1.55M trainable parameters**  

ğŸ’¾ **Memory footprint:**  
- FP32 â‰ˆ 6.2 MB (weights only)  

---

## ğŸš€ Getting Started

### 1. Prerequisites
- An **AMD GPU** compatible with the ROCm toolkit  
- **ROCm Toolkit** (5.0 or newer)  
- **CMake** (3.21 or newer)  
- A C++ compiler (`g++` or `clang++`)  
- `git` and `wget` for setup  

### 2. Clone the Repository
```bash
git clone git@github.com:aarnetalman/HipGPT.git
cd HipGPT
````

### 3. Download the Dataset

```bash
chmod +x scripts/download_data.sh
./scripts/download_data.sh
```

Creates a `data/` directory with `data.txt`.

### 4. Build the Project

```bash
mkdir build
cd build
cmake .. -DCMAKE_CXX_COMPILER=/usr/bin/hipcc
make
```

Produces two executables: `train_gpt` and `generate`.

---

## ğŸ‹ï¸ Training

Run from the project root:

```bash
chmod +x scripts/run_train.sh
./scripts/run_train.sh
```

Artifacts:

* `tokenizer.json` â€“ trained vocabulary
* `gpt_checkpoint.bin` â€“ trained weights

Example custom run:

```bash
./scripts/run_train.sh --steps 1000 --lr 1e-3
```

---

## âœï¸ Generating Text

```bash
./build/generate --prompt "To be, or not to be:"
```

Options:

* `--prompt "<text>"` (required)
* `--num_tokens N` (default: 50)
* `--top_k N` (default: 5)
* `--temp F` (default: 1.0)

Example:

```bash
./build/generate --prompt "My kingdom for a" --num_tokens 100 --top_k 50 --temp 0.8
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ build/                 # Build outputs
â”œâ”€â”€ data/                  # Training data
â”œâ”€â”€ scripts/               # Helper scripts
â”‚   â”œâ”€â”€ download_data.sh
â”‚   â””â”€â”€ run_train.sh
â”œâ”€â”€ include/               # Public headers
â”‚   â”œâ”€â”€ gpt_model.h
â”‚   â”œâ”€â”€ hip_kernels.h
â”‚   â”œâ”€â”€ tokenizer.h
â”‚   â””â”€â”€ transformer_layer.h
â”œâ”€â”€ src/                   # Source files
â”‚   â”œâ”€â”€ generate.cpp
â”‚   â”œâ”€â”€ gpt_model.cpp
â”‚   â”œâ”€â”€ hip_kernels.cpp
â”‚   â”œâ”€â”€ tokenizer.cpp
â”‚   â”œâ”€â”€ train_gpt.cpp
â”‚   â””â”€â”€ transformer_layer.cpp
â”œâ”€â”€ CMakeLists.txt         # Build config
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸ— Architecture Overview

* **BPE Tokenizer** â€“ converts raw text into integer IDs, trainable from scratch
* **Transformer Layer** â€“ multi-head self-attention + FFN with residuals and layer norm
* **GPT Model** â€“ embeddings + stacked transformer layers + final projection to vocab

---

## ğŸ“œ License

This project is licensed under the MIT License.

